[
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals RisingStack Blog We’re a full-stack software development agency focusing on JavaScript, DevOps, Microservices & Kubernetes. For more info visit risingstack.com, or feel free to contact us! * Search Search Tags edited reviewed Download & Update Node.js to the Latest Version! Node v21.1.0 Current / LTS v20.9.0 Direct Links Ferenc Hámori Direct download links to update to the latest Node.js versions: Node v21.1.0 / LTS v20.9.0 Node.js 21 is here with Websocket RisingStack Engineering The latest major version of Node.js has just released with a few new interesting experimental features and a lot of fixes and optimization. You can The Best JavaScript Frameworks: Pros and Cons Explained RisingStack Engineering In this article, we’ll focus on the most popular JavaScript frameworks, and explore why they’re either loved or disliked by developers. ChatGPT use case examples for programming Ferenc Hámori Learn how we use GPT-based products as tools in a professional webdev setting. AI Development Tools Compared – The Differences You’ll Need to Know RisingStack Engineering There are many different types of AI development tools available, but not all of them are created equal. Some tools are more suited for certain Kubernetes Interview Questions and Answers You’ll Need the Most RisingStack Engineering Are you currently preparing for a Kubernetes interview? If so, you’ll want to make sure you’re familiar with the questions and answers below at least. RedwoodJS vs. BlitzJS: The Future of Fullstack JavaScript Meta-Frameworks Tamas Kadlecsik RedwoodJS & BlitzJS are meta-frameworks that provide tooling for creating SPAs, server-side rendered pages & statically generated content, providing a CLI to generate e2e scaffolds. Argo CD Kubernetes Tutorial Janos Kubisch With this Argo CD Kubernetes tutorial you’ll learn to store credentials safely within your k8s cluster using a pull-based continous deployment tool. How to Deploy a Ceph Storage to Bare Virtual Machines RisingStack Engineering The main drawback of a Ceph storage is that you have to host and manage it yourself. In this post, we’ll check two different approaches of deploying Ceph. Async Await in Node.js – How to Master it? Tamas Kadlecsik Learn how to use async await in Node.js (async functions) to simplify your callback or Promise based application. Sometimes you do need Kubernetes! But how should you decide? Tamas Kadlecsik A case study where the adoption of Kubernetes has been heavily contested. Learn about our decision making process, and how we overcame k8s’s limitations! Distributed Load Testing with Jmeter Janos Kubisch Learn how to distribute and run Jmeter tests along multiple droplets on DigitalOcean using Terraform, Ansible, and bash scripting – to automate the process. Node.js Async Best Practices & Avoiding the Callback Hell tamas-hodi This post covers what tools and techniques you have at your disposal when handling Node.js asynchronous operations. Learn how to avoid the callback hell ! Mammogram Analysis with AI and User-Friendly Interfaces Janos Kubisch See how RisingStack and Semmelweis University created an image recognition system using deep convolutional neural networks and a user-friendly interface for healthcare workers fighting against breast cancer. RisingStack News (formerly Microservice Weekly) – a hand-curated newsletter RisingStack Engineering Microservice Weekly, our newsletter about microservices had a great run – we’ve carefully curated the best sources we could find and sent out more than Do your engineers do what you think they do? Tamas Kadlecsik This case study shows how we reformed a scale-up’s dev processes after uncovering severe discrepancies between the official and real way of getting things done. History of JavaScript on a Timeline RisingStack Engineering In the early 1990s, Brendan Eich needed a scripting language for web pages that would be easy to use, so he created one himself. « Previous Page1 Page2 Page3 … Page22 Next » *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals AI Development Tools Compared – The Differences You’ll Need to Know Last updated: February 2, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: RisingStack Engineering There are many different types of AI development tools available, but not all of them are created equal. Some tools are more suited for certain tasks than others, and it’s important to select the right tool for the job. Choosing the wrong tool can lead to frustration and wasted time, so it’s important to do your research before you start coding. There are many different types of AI development tools available, so there’s sure to be one that fits your needs. Common types of AI development tools include cloud-based platforms, open source software, and low code development tools. Cloud-based platforms are typically the most user friendly and allow you to build sophisticated models quickly. They offer a wide variety of features, such as data analysis tools, natural language processing capabilities, automatic machine learning models creation and pre-trained models that can be used for various tasks. Open-source software offers a great deal of flexibility and the ability to customize your AI model for specific tasks. However, using open source software requires coding knowledge and experience and is best suited for more experienced developers. Low code development tools allow you to create AI applications without having to write code. These tools allow developers of any skill level to quickly and easily create AI applications, eliminating the need for coding knowledge or experience. Of course, there are occasional overlaps, like cloud platforms using open-source technologies – but to find out all the similarities and differences, we’ll need to examine them. Let’s explore each one in further detail: Detectron 2 Detectron 2 is Facebook’s state-of-the-art object detection and segmentation library. It features a number of pre-trained models and baselines that can be used for a variety of tasks, and it also has cuda bindings that allow it to run on gpu for even faster training. Compared to its predecessor, Detectron 2 is much faster to train and can achieve better performance on a variety of benchmarks. It is also open source and written in python, making it easy to use and extend. Overall, Detectron 2 is an excellent choice for any object detection or segmentation task. The fact that it is built on PyTorch makes it very easy to share models between different use cases. For example, a model that is developed for research purposes can be quickly transferred to a production environment. This makes Detectron2 ideal for organizations that need to move quickly and efficiently between different use cases. In addition, the library’s ability to handle large-scale datasets makes it perfect for organizations that need to process large amounts of data. Overall, Detectron2 is an extremely versatile tool that can be used in a variety of different settings. Caffe Caffe is a deep learning framework for model building and optimisation. It was originally focused on vision applications, but it is now branching out into other areas such as sequences, reinforcement learning, speech, and text. Caffe is written in C++ and CUDA, with interfaces for python and mathlab. The community has built a number of models which are available at https://github.com/BVLC/caffe/wiki/Model-Zoo. Caffe is a powerful tool for anyone interested in deep learning. It features fast, well-tested code and a seamless switch between CPU and GPU – meaning that if you don’t have a GPU that supports CUDA, it automatically defaults to the CPU. This makes it a versatile tool for deep learning researchers and practitioners. The Caffe framework is also open source, so anyone can contribute to its development. Caffe offers the model definitions, optimization settings, and pre-trained weights so you can start right away. The BVLC models are licensed for unrestricted use, so you can use them in your own projects without any restrictions. Keras Keras is a deep learning framework that enables fast experimentation. It is based on Python and supports multiple backends, including TensorFlow, CNTK, and Theano. Keras includes specific tools for computer vision (KerasCV) and natural language processing (KerasNLP). Keras is open source and released under the MIT license. The idea behind Keras is to provide a consistent interface to a range of different neural network architectures, allowing for easy and rapid prototyping. It is also possible to run Keras models on top of other lower-level frameworks such as MXNet, Deeplearning4j, TensorFlow or Theano. Keras, like other similar tools, has the advantage of being able to run on both CPU and GPU devices with very little modification to the code. In addition, Keras includes a number of key features such as support for weight sharing and layer reuse, which can help to improve model performance and reduce training time. CUDA The CUDA toolkit is a powerful set of tools from NVIDIA for running code on GPUs. It includes compilers, libraries, and other necessary components for developing GPU-accelerated applications. The toolkit supports programming in Python, C, and C++, and it makes it easy to take advantage of the massive parallel computing power of GPUs. With the CUDA toolkit, you can accelerate your code to run orders of magnitude faster than on a CPU alone. Whether you’re looking to speed up machine learning algorithms or render complex 3D graphics, the CUDA toolkit can help you get the most out of your NVIDIA GPUs. In the context of fraud detection, the CUDA toolkit can be used to train graph neural networks (GNNs) on large datasets in an efficient manner. This allows GNNs to learn from more data, which can lead to improved performance. In addition, the CUDA toolkit can be used to optimize the inference process, which is important for real-time applications such as fraud detection, which is a critical application for machine learning. Many techniques struggle with fraud detection because they cannot easily identify patterns that span multiple transactions. However, GNNs are well-suited to this task due to their ability to aggregate information from the local neighborhood of a transaction. This enables them to identify larger patterns that may be missed by traditional methods. TensorFlow TensorFlow is an open-source platform for machine learning that offers a full pipeline from model building to deployment. It has a large collection of pre-trained models and supports a broad range of programming languages including Javascript, Python, Android, Swift, C++, and Objective C. TensorFlow uses the Keras API and also supports CUDA for accelerated training on NVIDIA GPUs. In addition to providing tools for developers to build and train their own models, TensorFlow also offers a wide range of resources such as tutorials and guides. TensorFlow.js is a powerful tool that can be used to solve a variety of problems. In the consumer packaged goods (CPG) industry, one of the most common problems is real-time and offline SKU detection. This problem is often caused by errors in manually inputting data, such as when a product is scanned at a store or when an order is placed online. TensorFlow.js can be used to create a solution that would automatically detect and correct these errors in real time, as well as provide offline support for cases where a connection is not available. This can greatly improve the efficiency of the CPG industry and reduce the amount of waste caused by incorrect data input. PyTorch PyTorch is a powerful machine learning framework that allows developers to create sophisticated applications for computer vision, audio processing, and time series analysis. The framework is based on the popular Python programming language, and comes with a large number of libraries and frameworks for easily creating complex models and algorithms. PyTorch also supports bindings for c++ and java, making it a great option for cross-platform development. In addition, the framework includes CUDA support for accelerated computing on NVIDIA GPUs. And finally, PyTorch comes with a huge collection of pre-trained models that can be used for quickly building sophisticated applications. PyTorch’s ease of use and flexibility make it a popular choice for researchers and developers alike. The PyTorch framework is known to be convenient and flexible, with examples covering reinforcement learning, image classification, and natural language processing as the more common use cases. As a result, it is no surprise that the framework has been gaining popularity in recent years. Thanks to its many features and benefits, PyTorch looks poised to become the go-to framework for deep learning in the years to come. Apache MXNet MXNet is an open-source deep learning framework that allows you to define, train, and deploy deep neural networks on a wide array of devices, from cloud infrastructure to mobile devices. It’s scalable, allowing for fast model training, and supports a flexible programming model and multiple languages. It’s built on a dynamic dependency scheduler that automatically parallelizes both symbolic and imperative operations on the fly. A graph optimization layer makes symbolic execution fast and memory efficient. The MXNet library is portable and lightweight. It’s accelerated with the NVIDIA Pascal™ GPUs and scales across multiple GPUs and multiple nodes, allowing you to train models faster. Whether you’re looking to build state-of-the-art models for image classification, object detection, or machine translation, MXNet is the tool for you. Horovod Horovod is a distributed training framework for deep learning that supports TensorFlow, Keras, PyTorch, and Apache MXNet. It is designed to make distributed training easy to use and efficient. Horovod uses a message passing interface to communicate between nodes, and each node runs a copy of the training script. The framework handles the details of communication and synchronization between nodes so that users can focus on their model. Horovod also includes a number of optimizations to improve performance, such as automatically fusing small tensors together and using hierarchical allreduce to reduce network traffic. For Uber’s data scientists, the process of installing TensorFlow was made even more challenging by the fact that different teams were using different releases of the software. The team wanted to find a way to make it easier for all teams to use the ring-allreduce algorithm, without requiring them to upgrade to the latest version of TensorFlow or apply patches to their existing versions. The solution was to create a stand-alone package called Horovod. This package allowed the team to cut the time required to install TensorFlow from about an hour to a few minutes, depending on the hardware. As a result, Horovod has made it possible for Uber’s data scientists to spend less time installing software and more time doing what they do best. Oracle AI Oracle AI is a suite of artificial intelligence services that can be used to build, train and deploy models. The services include natural language processing, chat bots / customer support, text-to-speech, speech-to-text, object detection for images and data mining. Oracle AI offers pre-configured vms with access to GPUs. The service can be used to build models for anomaly detection, analytics and data mining. Oracle AI is a powerful tool that can be used to improve your business. Children’s Medical Research Institute (CMRI) is a not-for-profit organisation dedicated to improving the health of children through medical research. CMRI moved to Oracle Cloud Infrastructure (OCI) as its preferred cloud platform. This move has helped the institute take advantage of big data and machine learning capabilities to automate routine database tasks, database consolidation, operational reporting, and batch data processing. Overall, the switch to OCI has been a positive move for CMRI, and one that is sure to help the institute continue its important work. H2O H2O is a powerful open source AI platform that is used by companies all over the world to improve their customer support, marketing, and data mining efforts. The software provides a wide range of features that make it easy to collect and analyze customer data, identify anomalies, and create chat bots that can provide an engaging customer experience. H2O is constantly evolving, and the company behind it is always introducing new features and improvements. For example, it can be used to create an intelligent cash management system that predicts cash demand and helps to optimize ATM operations. It can also help information security teams reduce risk by identifying potential threats and vulnerabilities in real time. In addition, H2O.AI can be used to transform auditing from quarterly to real-time, driving audit quality, accuracy and reliability. Alibaba Cloud Alibaba Cloud is a leading provider of cloud computing services. Its products include machine learning, natural language processing, data mining, and analytics. Alibaba Cloud’s machine learning platform offers a variety of pre-created algorithms that can be used for tasks such as data mining, anomaly detection, and predictive maintenance. The platform also provides tools for training and deploying machine learning models. Alibaba Cloud’s natural language processing products offer APIs for text analysis, voice recognition, and machine translation. The company’s data mining and analytics products provide tools for exploring and analyzing data. Alibaba Cloud also offers products for security, storage, and networking. Alibaba, the world’s largest online and mobile commerce company, uses intelligent recommendation algorithms to drive sales using personalized customer search suggestions on its Tmall homepage and mobile app. The system takes into account a customer’s purchase history, browsing behavior, and social interactions when making recommendations. Alibaba has found that this approach leads to increased sales and higher customer satisfaction. In addition to search suggestions, the system also provides personalized product recommendations to customers based on their past behavior. This has resulted in increased sales and engagement on the platform. Alibaba is constantly tweaking and improving its algorithms to ensure that it is providing the most relevant and useful data to its users. IBM Watson IBM Watson is a powerful artificial intelligence system that has a range of applications in business and industry. One of the most important functions of Watson is its ability to process natural language. This enables it to understand human conversation and respond in a way that sounds natural. This capability has been used to develop chatbots and customer support systems that can replicate human conversation. In addition, Watson’s natural language processing capabilities have been used to create marketing campaigns that can target specific demographics. Another key application of Watson is its ability to detect anomalies. This makes it an essential tool for monitoring systems and identifying potential problems. As a result, IBM Watson is a versatile and valuable artificial intelligence system with a wide range of applications. IBM Watson is employed in nearly every industry vertical, as well as in specialized application areas such as cybersecurity. This technology is often used by a company’s data analytics team, but Watson has become so user friendly that it is also easily used by end users such as physicians or marketers. Azure AI Azure AI is a suite of services from Microsoft that helps you build, optimize, train, and deploy models. You can use it for object detection in images and video, natural language processing, chatbots and customer support, text-to-speech, speech-to-text, data mining and analytics, and anomaly detection. Azure AI also provides pre-configured virtual machines so you can get started quickly and easily. Whether you’re an experienced data scientist or just getting started with machine learning, Azure AI can help you achieve your goals. With the rapid pace of technological advancement, it is no surprise that the aviation industry is constantly evolving. One of the leading companies at the forefront of this change is Airbus. The company has unveiled two new innovations that utilize Azure AI solutions to revolutionize pilot training and predict aircraft maintenance issues. Google AI Google AI is a broad set of tools and services that helps you build, deploy, and train models, as well as to take advantage of pre-trained models. You can use it to detect objects in images and video, to perform natural language processing tasks such as chat bots or customer support, to translate text, and to convert text-to-speech or speech-to-text. Additionally, Google AI can be used for data mining and analytics, as well as for anomaly detection. All of these services are hosted on Google Cloud Platform, which offers a variety of options for GPU-accelerated computing, pre-configured virtual machines, and TensorFlow hosting. UPS and Google Cloud Platform were able to develop routing software that has had a major impact on the company’s bottom line. The software takes into account traffic patterns, weather conditions, and the location of UPS facilities, in order to calculate the most efficient route for each driver. As a result, UPS has saved up to $400 million a year, and reduced its fuel consumption by 10 million gallons. In addition, the software has helped to improve customer satisfaction by ensuring that packages are delivered on time. AWS AI Amazon Web Services offers a variety of AI services to help developers create intelligent applications. With pre-trained models for common use cases, AWS AI makes it easy to get started with machine learning. For images and video, the object detection service provides accurate labels and coordinates. Natural language processing can be used for chat bots and customer support, as well as translation. Text-to-speech and speech-to-text are also available. AI powered search provides relevant results from your data. Pattern recognition can be used for code review and monitoring. And data mining and analytics can be used for anomaly detection. AWS AI also offers hosted GPUs and pre-configured vms. With so many powerful features, Amazon Web Services is the perfect platform for developing AI applications. Formula 1 is the world’s most popular motorsport, with hundreds of millions of fans worldwide. The sport has been at the forefront of technological innovation for decades, and its use of data and analytics has been central to its success. Teams have long used on-premises data centers to store and process large amounts of data, but the sport is now accelerating its transformation to the cloud. Formula 1 is moving the vast majority of its infrastructure to Amazon Web Services (AWS), and standardizing on AWS’s machine-learning and data-analytics services. This will enable Formula 1 to enhance its race strategies, data tracking systems, and digital broadcasts through a wide variety of AWS services—including Amazon SageMaker, AWS Lambda, and AWS’s event-driven serverless computing service. By using these services, Formula 1 will be able to deliver new race metrics that will change the way fans and teams experience racing. Conclusion Choosing the right AI development tool can be difficult. This article has provided a comparison of some of the most popular tools on the market. Each tool has its own strengths and weaknesses, so it is important to decide which one will best suit your needs. *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/ai-development-tools-compared/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals Mammogram Analysis with AI and User-Friendly Interfaces Last updated: April 25, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: Janos Kubisch We are excited to share a new project we have been working on in collaboration with Hungary’s leading medical research university – Semmelweis. This project focuses on using artificial intelligence and image recognition technologies to improve the accuracy and efficiency of breast cancer screenings. The Power of AI in Detecting Breast Cancer Early detection and prevention are crucial in the fight against breast cancer, and recent advancements in technology have made it possible for healthcare workers to receive computer assistance in examining mammograms and identifying problematic areas. The integration of machine learning and image recognition technologies in the medical field has the potential to revolutionize breast cancer screening, making it more accurate and efficient. However, the widespread adoption of these artificial intelligence-based solutions will not be possible without good products with great user experience. A user-friendly interface will make it easier for healthcare workers to use these technologies and improve patient outcomes, making it a crucial component in the fight against breast cancer. As a recognized leader in medical research, Semmelweis University has a long-standing reputation for producing cutting-edge advancements in the field. We are proud to have the opportunity to partner with such an esteemed institution and contribute to their ongoing efforts to improve medical outcomes and advance the field of medicine. Implementing AI with a User-Friendly Interface The primary objective of this project was to make the workings of the algorithm more visually accessible to medical professionals. The goal was to design a platform that could be run on any device with network connectivity, either through a standalone application or using Docker technology. This would allow us to demonstrate the algorithm’s capabilities on-site, making it easier for those considering its use to get a hands-on understanding of its capabilities. During the image processing and annotation stage, the application provides real-time feedback in the form of a visual animation and progress bar. This helps users keep track of the analysis as it progresses and gives them a sense of the speed and efficiency of the algorithm. Once all the images have been processed, the application highlights those that show an increased risk, based on the annotations, in an interactive gallery. This gallery provides a clear and easy-to-understand representation of the algorithm’s results, making it a valuable tool for both users and potential adopters. The software is intentionally slowed down for presentation purposes. It’s much, much faster, of course. The Algorithm: Using Faster-RCNN and VGG16 The image detection algorithm at the core of this project uses a state-of-the-art region-based deep convolutional neural network called Faster-R-CNN. This powerful model was specifically designed for object detection and proved to be an effective tool for identifying problematic regions in mammograms. The base network used in the model was VGG16, a highly regarded 16-layer deep convolutional neural network that can be easily obtained from the PyTorch website. To make the algorithm even more effective, we further trained it to detect two different types of objects in mammogram images: benign and malignant lesions. The output of the algorithm is not just a simple diagnosis, but a comprehensive report that includes a score reflecting the confidence level in the diagnosis for each detected lesion. The algorithm also generates a modified image that clearly highlights the locations of the detected lesions by overlaying bounding boxes on the original mammogram. This makes it easy for healthcare workers to understand and interpret the results of the analysis. The Backend – Frontend Integration The backend is responsible for managing the running of the algorithm and ensuring that the results are promptly sent to the frontend once the analysis of an image is completed. The input images are first sent to the frontend, where they are overlaid with a scanline animation, providing a visual indication that the analysis is underway. As soon as the results are available, they are displayed on the frontend as a simple red/green overlay and a small animation before transitioning to the next image. To avoid any potential performance issues, the algorithm is run in serial for all images, as running it in parallel would quickly cause problems when using only the CPU and system memory. However, the CUDA Python SDK provides the ability to automatically use the CPU if a dedicated GPU cannot be found, making it possible to use the algorithm even on basic devices, albeit with reduced efficiency. When a suitable nVidia GPU is available, the algorithm can be run in larger batches, providing much faster results. To get the results back to the frontend, we used Socket.io, as it allows for real-time communication between the backend and frontend, allowing us to directly push data from the backend to the frontend as soon as the algorithm finishes processing an image. The images that have a confidence score below a certain threshold are considered “negative,” indicating that they are likely healthy. These images are presented in a distinctive way, with a small scale-out-scale-in animation. This animation is achieved through the use of CSS animation, utilizing the scale transform function. Deploying the Application with Docker The entire application is packaged in a docker image, making it more accessible and easier to run and distribute. This approach has a number of advantages, one of which is the ability to deploy the application to a cloud service, which opens up the possibility of accessing it from anywhere with an internet connection. However, it is important to consider the architecture of the system you will run the application on, as this can impact which base image you will choose. For example, on M1 Macs, arm64 images are required, and attempting to run other images may result in errors. By utilizing a docker image, the project benefits from the portability and compatibility provided by this technology, allowing for seamless deployment and usage across different platforms. Node, Express and React under the hood In the implementation of this project, we chose to utilize a combination of Node.js and Express for the backend and React for the frontend. This choice was made based on the strengths and capabilities of these technologies, which provided the ideal foundation for the application’s needs. However, it is worth noting that this design may not be the only possible solution, and the application could also be implemented as an Electron app, which is a popular framework for creating desktop applications. This versatility highlights the flexibility of the project and its ability to adapt to different environments and technologies. The key is to find the right tool for the job, and in this case, we found that Node.js, Express, and React provided the optimal solution for our needs. How RisingStack can help with your AI project As businesses and corporations look to harness the power of Artificial Intelligence, there’s a growing demand for software development companies that can help implement these AI models and create web-based user interfaces to accompany them. That’s where RisingStack can help you. In the past couple of months, we created several custom AI solutions for businesses and institutions of all sizes, just to name a few: Using AI to automatically generate product names and descriptions for webshop engines. Creating easy-read text for children with disabilities Sentiment analysis and automatic answers in the hospitality industry. Pinpointing breast cancer using neural networks. Whether you’re looking to create a custom AI model to help streamline your business processes, or you’re looking to build a web-based UI that provides users with a more engaging and interactive experience, we have the skills and expertise *. Get in touch with us to learn more about how we can help you implement AI models and create web-based UI-s that drive results for your business. *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/ai-healthcare-semmelweis-risingstack-case-study/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals Argo CD Kubernetes Tutorial Last updated: September 11, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: Janos Kubisch Usually, when devs set up a CI/CD pipeline for an application hosted on KubernetesKubernetes (often abbreviated as K8s) offers a framework to run distributed systems efficiently. It's a platform that helps managing containerized workloads and services, and even takes care of scaling. Google open-sourced it in 2014., they handle both the CI and CD parts in one task runner, such as CircleCI or Travis CI. These services offer push-based updates to your deployments, which means that credentials for the code repo and the deployment target must be stored with these services. This method can be problematic if the service gets compromised, e.g. as it happened to CodeShip. Even using services such as GitLab CI and GitHub Actions requires that credentials for accessing your cluster be stored with them. If you’re employing GitOps, to take advantage of using the usual Push to repo -> Review Code -> Merge Code sequence for managing your infrastructure configuration as well, this would also mean access to your whole infrastructure. Learn When to Use Kubernetes: Get Our Case Study! Enter your email to subscribe to our newsletter and we’ll send you a link to download the case study. DOWNLOAD NOW Luckily there are tools to help us with these issues. Two of the most known are Argo CD and Flux. They allow credentials to be stored within your Kubernetes cluster, where you have more control over their security. They also offer pull-based deployment with drift detection. Both of these tools solve the same issues, but tackle them from different angles. Here, we’ll take a deeper look at Argo CD out of the two. What is Argo CD Argo CD is a continuous deployment tool that you can install into your Kubernetes cluster. It can pull the latest code from a git repository and deploy it into the cluster – as opposed to external CD services, deployments are pull-based. You can manage updates for both your application and infrastructure configuration with Argo CD. Advantages of such a setup include being able to use credentials from the cluster itself for deployments, which can be stored in secrets or a vault. Preparation To try out Argo CD, we’ve also prepared a test project that we’ll deploy to Kubernetes hosted on DigitalOcean. You can grab the example project from our GitLab repository here: https://gitlab.com/risingstack-org/argocd-demo/ Forking the repo will allow you to make changes for yourself, and it can be set up later in Argo CD as the deployment source. Get doctl from here: https://docs.digitalocean.com/reference/doctl/how-to/install/ Or, if you’re using a mac, from Homebrew: brew install doctl You can use any Kubernetes provider for this tutorial. The two requirements are having a Docker repository and a Kubernetes cluster with access to it. For this tutorial, we chose to go with DigitalOcean for the simplicity of its setup, but most other platforms should work just fine. We’ll focus on using the web UI for the majority of the process, but you can also opt to use the `doctl` cli tool if you wish. `doctl` can mostly replace `kubectl` as well. `doctl` will only be needed to push our built docker image to the repo that our deployment will have access to. Helm is a templating engine for Kubernetes. It allows us to define values separately from the structure of the yaml files, which can help with access control and managing multiple environments using the same template. You can grab Helm here: https://github.com/helm/helm/releases Or via Homebrew for mac users: brew install helm Download the latest Argo CD version from https://github.com/argoproj/argo-cd/releases/latest If you’re using a mac, you can grab the cli tools from Homebrew: brew install argocd DigitalOcean Setup After logging in, first, create a cluster using the “Create” button on the top right, and selecting Kubernetes. For the purposes of this demo, we can just go with the smallest cluster with no additional nodes. Be sure to choose a data center close to you. Preparing the demo app You can find the demo app in the node-app folder in the repo you forked. Use this folder for the following steps to build and push the docker image to the GitLab registry: docker login registry.gitlab.com docker build . -t registry.gitlab.com/<substiture repo name here>/demo-app-1 docker push registry.gitlab.com/<substiture repo name here>/demo-app-1 GitLab offers a free image registry with every git repo – even free tier ones. You can use these to store your built image, but be aware that the registry inherits the privacy setting of the git repo, you can’t change them separately. Once the image is ready, be sure to update the values.yaml file with the correct image url and use helm to generate the resources.yaml file. You can then deploy everything using kubectl: helm template -f \"./helm/demo-app/values.yaml\" \"./helm/demo-app\" > \"./helm/demo-app/resources/resources.yaml\" kubectl apply -f helm/demo-app/resources/resources.yaml The only purpose of these demo-app resources’ is to showcase the ArgoCD UI capabilities, that’s why it also contains an Ingress resource as a plus. Install Argo CD into the cluster Argo CD provides a yaml file that installs everything you’ll need and it’s available online. The most important thing here is to make sure that you install it into the `argocd` namespace, otherwise, you’ll run into some errors later and Argo CD will not be usable. kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml From here, you can use Kubernetes port-forwarding to access the UI of Argo CD: kubectl -n argocd port-forward svc/argocd-server 8080:443 This will expose the service on localhost:8080 – we will use the UI to set up the connection to GitLab, but it could also be done via the command line tool. Argo CD setup To log in on the UI, use `admin` as username, and the password retrieved by this command: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d Once you’re logged in, connect your fork of the demo app repo from the Repositories inside the Settings menu on the left side. Here, we can choose between ssh and https authentication – for this demo, we’ll use https, but for ssh, you’d only need to set up a key pair for use. Create an API key on GitLab and use it in place of a password alongside your username to connect the repo. An API key allows for some measure of access control as opposed to using your account password. After successfully connecting the repository, the only thing left is to set up an Application, which will take care of synchronizing the state of our deployment with that described in the GitLab repo. You’ll need to choose a branch or a tag to use to monitor. Let’s choose the master branch for now – it should contain the latest stable code anyway. Setting the sync policy to automatic allows for automatic deployments when the git repo is updated, and also provides automatic pruning and self-healing capabilities. Be sure to set the destination cluster to the one available in the dropdown and use the `demo` namespace. If everything is set correctly, Argo CD should now start syncing the deployment state. Features of Argo CD From the application view, you can now see the different parts that comprise our demo application. Clicking on any of these parts allows for checking the diff of the deployed config, and the one checked into git, as well as the yaml files themselves separately. The diff should be empty for now, but we’ll see it in action once we make some changes or if you disable automatic syncing. You also have access to the logs from the pods here, which can be quite useful – logs are not retained between different pod instances, which means that they are lost on the deletion of a pod, however. It is also possible to handle rollbacks from here, clicking on the “History and Rollback” button. Here, you can see all the different versions that have been deployed to our cluster by commit. You can re-deploy any of them using the … menu on the top right, and selecting “Redeploy” – this feature needs automatic deployment to be turned off. However, you’ll be prompted to do so here. These should cover the most important parts of the UI and what is available in Argo CD. Next up, we’ll take a look at how the deployment update happens when code changes on GitLab. Updating the deployment With the setup done, any changes you make to the configuration that you push to the master branch should be reflected on the deployment shortly after. A very simple way to check out the updating process is to bump up the `replicaCount` in values.yaml to 2 (or more), and run the helm command again to generate the resources.yaml. Then, commit and push to master and monitor the update process on the Argo CD UI. You should see a new event in the demo-app events, with the reason `ScalingReplicaSet`. You can double-check the result using kubectl, where you should now see two instances of the demo-app running: kubectl -n demo get pod There is another branch prepared in the repo, called second-app, which has another app that you can deploy, so you can see some more of the update process and diffs. It is quite similar to how the previous deployment works. First, you’ll need to merge the second-app branch into master – this will allow the changes to be automatically deployed, as we set it up already. Then, from the node-app-2 folder, build and push the docker image. Make sure to have a different version tag for it, so we can use the same repo! docker build . -t registry.gitlab.com/<substitute repo name here>/demo-app-2 docker push registry.gitlab.com/<substitute repo name here>/demo-app-2 You can set deployments to manual for this step, to be able to take a better look at the diff before the actual update happens. You can do this from the sync settings part of `App details`. Generate the updated resources file afterwards, then commit and push it to git to trigger the update in Argo CD: helm template -f \"./helm/demo-app/values.yaml\" \"./helm/demo-app\" > \"./helm/demo-app/resources/resources.yaml\" This should result in a diff appearing `App details` -> `Diff` for you to check out. You can either deploy it manually or just turn auto-deploy back. ArgoCD safeguards you from those resource changes that are drifting from the latest source-controlled version of your code. Let’s try to manually scale up the deployment to 5 instances: Get the name of the replica set: kubectl -n demo get rs Scale it to 5 instances: kubectl -n demo scale --replicas=5 rs/demo-app-<number> If you are quick enough, you can catch the changes applied on the ArgoCD Application Visualization as it tries to add those instances. However, ArgoCD will prevent this change, because it would drift from the source controlled version of the deployment. It also scales the deployment down to the defined value in the latest commit (in my example it was set to 3). The downscale event can be found under the `demo-app` deployment events, as shown below: From here, you can experiment with whatever changes you’d like! Finishing our ArgoCD Kubernetes Tutorial This was our quick introduction to using ArgoCD, which can make your GitOps workflow safer and more convenient. Stay tuned, as we’re planning to take a look at the other heavy-hitter next time: Flux. This article was written by Janos Kubisch, senior engineer at RisingStack. *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/argo-cd-kubernetes-tutorial/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals The Best JavaScript Frameworks: Pros and Cons Explained Last updated: March 17, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: RisingStack Engineering There are a lot of different JavaScript frameworks out there, and it can be tough to keep track of them all. In this article, we’ll focus on the most popular ones, and explore why they’re either loved or disliked by developers. React React is a JavaScript library for building user interfaces. It is maintained by Facebook and a community of individual developers and companies. React can be used as a base in the development of single-page or mobile applications. However, React is only concerned with rendering data to the DOM, and so creating React apps usually requires the use of additional libraries for state management, routing, and interaction with an API. React is also used for building reusable UI components. In that sense, it works much like a JavaScript framework such as Angular or Vue. However, React components are typically written in a declarative manner rather than using imperative code, making them easier to read and debug. Because of this, many developers prefer to use React for building UI components even if they are not using it as their entire front-end framework. Advantages: React is fast and efficient because it uses a virtual DOM rather than manipulating the real DOM. React is easy to learn because of its declarative syntax and clear documentation. React components are reusable, making code maintenance easier. Disadvantages: React has a large learning curve because it is a complex JavaScript library. React is not a full-fledged framework, and so it requires the use of additional libraries for many tasks. Next.js Next.js is a javascript library that enables server-side rendering for React applications. This means that next.js can render your React application on the server before sending it to the client. This has several benefits. First, it allows you to pre-render components so that they are already available on the client when the user requests them. Second, it enables better SEO for your React application by allowing crawlers to index your content more easily. Finally, it can improve performance by reducing the amount of work that the client has to do in order to render the page. Here’s why developers like Next.js: Next.js makes it easy to get started with server-side rendering without having to do any configuration. Next.js automatically code splits your application so that each page is only loaded when it is requested, which can improve performance. Disadvantages: If you’re not careful, next.js can make your application codebase more complex and harder to maintain. Some developers find the built-in features of next.js to be opinionated and inflexible. Vue.js Vue.js is an open-source JavaScript framework for building user interfaces and single-page applications. Unlike other frameworks such as React and Angular, Vue.js is designed to be lightweight and easy to use. The Vue.js library can be used in conjunction with other libraries and frameworks, or can be used as a standalone tool for creating front-end web applications. One of the key features of Vue.js is its two-way data binding, which automatically updates the view when the model changes, and vice versa. This makes it an ideal choice for building dynamic user interfaces. In addition, Vue.js comes with a number of built-in features such as a templating system, a reactivity system, and an event bus. These features make it possible to create sophisticated applications without having to rely on third-party libraries. As a result, Vue.js has become one of the most popular JavaScript frameworks in recent years. Advantages: Vue.js is easy to learn due to its small size and clear documentation. Vue.js components are reusable, which makes code maintenance easier. Vue.js applications are very fast due to the virtual DOM and async component loading. Disadvantages: While Vue.js is easy to learn, it has a large learning curve if you want to master all its features. Vue.js does not have as many libraries and tools available as some of the other frameworks. Angular Angular is a JavaScript framework for building web applications and apps in JavaScript, html, and Typescript. Angular is created and maintained by Google. Angular provides two-way data binding, so that changes to the model are automatically propagated to the view. It also provides a declarative syntax that makes it easy to build dynamic UIs. Finally, Angular provides a number of useful built-in services, such as HTTP request handling, and support for routing and templates. Advantages: Angular has a large community and many libraries and tools available. Angular is easy to learn due to its well-organized documentation and clear syntax. Disadvantages: While Angular is easy to learn, it has a large learning curve if you want to master all its features. Angular is not as lightweight as some of the other frameworks. Svelte In a nutshell, Svelte is a JavaScript framework similar to React, Vue, or Angular. However, where those frameworks use virtual DOM (Document Object Model) diffing to figure out what changed between views, Svelte uses a technique called DOM diffing. This means that it only updates the parts of the DOM that have changed, making for a more efficient rendering process. In addition, Svelte also includes some built-in optimizations that other frameworks do not, such as automatically batching DOM updates and code-splitting. These features make Svelte a good choice for high-performance applications. Advantages: Svelte has built-in optimizations that other frameworks do not, such as code-splitting. Svelte is easy to learn due to its clear syntax and well-organized documentation. Disadvantages: While Svelte is easy to learn, it has a large learning curve if you want to master all its features. Svelte does not have as many libraries and tools available as some of the other frameworks. Gatsby Gatsby is a free and open-source framework based on React that helps developers build blazing fast websites and apps. It uses cutting edge technologies to make the process of building websites and applications more efficient. One of its key features is its ability to prefetch resources so that they are available instantaneously when needed. This makes Gatsby websites extremely fast and responsive. Another benefit of using Gatsby is that it allows developers to use GraphQL to query data from any source, making it easy to build complex data-driven applications. In addition, Gatsby comes with a number of plugins that make it even easier to use, including ones for SEO, analytics, and image optimization. All of these factors make Gatsby an extremely popular choice for building modern websites and applications. Advantages: Gatsby websites are extremely fast and responsive due to its use of prefetching. Gatsby makes it easy to build complex data-driven applications due to its support for GraphQL. Gatsby comes with a number of plugins that make it even easier to use. Disadvantages: While Gatsby is easy to use, it has a large learning curve if you want to master all its features. Gatsby does not have as many libraries and tools available as some of the other frameworks. Nuxt.js Nuxt.js is a progressive framework for building JavaScript applications. It is based on Vue.js and comes with a set of tools and libraries that make it easy to create universal applications that can be rendered on server-side and client-side. Nuxt.js also provides a way to handle asynchronous data and routing, which makes it perfect for building highly interactive applications. In addition, Nuxt.js comes with a CLI tool that makes it easy to scaffold new projects and build, run, and test them. With Nuxt.js, you can create impressive JavaScript applications that are fast, reliable, and scalable. Advantages: Nuxt.js is easy to use and extend. Nuxt.js applications are fast and responsive due to server-side rendering. Disadvantages: While Nuxt.js is easy to use, it has a large learning curve if you want to master all its features. Nuxt.js does not have as many libraries and tools available as some of the other frameworks. Ember.js Ember.js is known for its conventions over configuration approach which makes it easier for developers to get started with the framework. It also features built-in libraries for common tasks such as data persistence and routing which makes development faster. Although Ember.js has a steep learning curve, it provides developers with a lot of flexibility and power to create rich web applications. If you’re looking for a front-end JavaScript framework to build SPAs, Ember.js is definitely worth considering. Advantages: Ember.js uses conventions over configuration which makes it easier to get started with the framework. Ember.js has built-in libraries for common tasks such as data persistence and routing. Ember.js provides developers with a lot of flexibility and power to create rich web applications. Disadvantages: Ember.js has a steep learning curve. Ember.js does not have as many libraries and tools available as some of the other frameworks. Backbone.js Backbone.js is a lightweight JavaScript library that allows developers to create single-page applications. It is based on the Model-View-Controller (MVC) architecture, which means that it separates data and logic from the user interface. This makes code more maintainable and scalable, as well as making it easier to create complex applications. Backbone.js also includes a number of features that make it ideal for developing mobile applications, such as its ability to bind data to HTML elements and its support for touch events. As a result, Backbone.js is a popular choice for developers who want to create fast and responsive applications. Advantages: Backbone.js is lightweight and only a library, not a complete framework. Backbone.js is easy to learn and use. Backbone.js is very extensible with many third-party libraries available. Disadvantages: Backbone.js does not offer as much built-in functionality as some of the other frameworks. Backbone.js has a smaller community than some of the other frameworks. Conclusion In conclusion, while there are many different JavaScript frameworks to choose from, the most popular ones remain relatively stable. Each has its own benefits and drawbacks that developers must weigh when making a decision about which one to use for their project. While no framework is perfect, each has something to offer that can make development easier or faster. Everyone should consider the specific needs of their project when choosing a framework, as well as the skills of their team and the amount of time they have to devote to learning a new framework. By taking all of these factors into account, you can choose the best JavaScript framework for your project! *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/best-javascript-frameworks/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals How to Deploy a Ceph Storage to Bare Virtual Machines Last updated: February 2, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: RisingStack Engineering Ceph is a freely available storage platform that implements object storage on a single distributed computer cluster and provides interfaces for object-, block- and file-level storage. Ceph aims primarily for completely distributed operation without a single point of failure. Ceph storage manages data replication and is generally quite fault-tolerant. As a result of its design, the system is both self-healing and self-managing. Ceph has loads of benefits and great features, but the main drawback is that you have to host and manage it yourself. In this post, we’ll check two different approaches of virtual machine deployment with Ceph. Anatomy of a Ceph cluster Before we dive into the actual deployment process, let’s see what we’ll need to fire up for our own Ceph cluster. There are three services that form the backbone of the cluster ceph monitors (ceph-mon) maintain maps of the cluster state and are also responsible for managing authentication between daemons and clients managers (ceph-mgr) are responsible for keeping track of runtime metrics and the current state of the Ceph cluster object storage daemons (ceph-osd) store data, handle data replication, recovery, rebalancing, and provide some ceph monitoring information. Additionally, we can add further parts to the cluster to support different storage solutions metadata servers (ceph-mds) store metadata on behalf of the Ceph Filesystem rados gateway (ceph-rgw) is an HTTP server for interacting with a Ceph Storage Cluster that provides interfaces compatible with OpenStack Swift and Amazon S3. There are multiple ways of deploying these services. We’ll check two of them: first, using the ceph/deploy tool, then a docker-swarm based vm deployment. Let’s kick it off! Ceph Setup Okay, a disclaimer first. As this is not a production infrastructure, we’ll cut a couple of corners. You should not run multiple different Ceph demons on the same host, but for the sake of simplicity, we’ll only use 3 virtual machines for the whole cluster. In the case of OSDs, you can run multiple of them on the same host, but using the same storage drive for multiple instances is a bad idea as the disk’s I/O speed might limit the OSD daemons’ performance. For this tutorial, I’ve created 4 EC2 machines in AWS: 3 for Ceph itself and 1 admin node. For ceph-deploy to work, the admin node requires passwordless SSH access to the nodes and that SSH user has to have passwordless sudo privileges. In my case, as all machines are in the same subnet on AWS, connectivity between them is not an issue. However, in other cases editing the hosts file might be necessary to ensure proper connection. Depending on where you deploy Ceph security groups, firewall settings or other resources have to be adjusted to open these ports 22 for SSH 6789 for monitors 6800:7300 for OSDs, managers and metadata servers 8080 for dashboard 7480 for rados gateway Without further ado, let’s start deployment. Ceph Storage Deployment Install prerequisites on all machines $ sudo apt update $ sudo apt -y install ntp python For Ceph to work seamlessly, we have to make sure the system clocks are not skewed. The suggested solution is to install ntp on all machines and it will take care of the problem. While we’re at it, let’s install python on all hosts as ceph-deploy depends on it being available on the target machines. Prepare the admin node $ ssh -i ~/.ssh/id_rsa -A ubuntu@13.53.36.123 As all the machines have my public key added to known_hosts thanks to AWS, I can use ssh agent forwarding to access the Ceph machines from the admin node. The first line ensures that my local ssh agent has the proper key in use and the -A flag takes care of forwarding my key. $ wget -q -O- 'https://download.ceph.com/keys/release.asc' | sudo apt-key add - echo deb https://download.ceph.com/debian-nautilus/ $(lsb_release -sc) main | sudo tee /etc/apt/sources.list.d/ceph.list $ sudo apt update $ sudo apt -y install ceph-deploy We’ll use the latest nautilus release in this example. If you want to deploy a different version, just change the debian-nautilus part to your desired release (luminous, mimic, etc.). $ echo \"StrictHostKeyChecking no\" | sudo tee -a /etc/ssh/ssh_config > /dev/null OR $ ssh-keyscan -H 10.0.0.124,10.0.0.216,10.0.0.104 >> ~/.ssh/known_hosts Ceph-deploy uses SSH connections to manage the nodes we provide. Each time you SSH to a machine that is not in the list of known_hosts (~/.ssh/known_hosts), you’ll get prompted whether you want to continue connecting or not. This interruption does not mesh well with the deployment process, so we either have to use ssh-keyscan to grab the fingerprint of all the target machines or disable the strict host key checking outright. 10.0.0.124 ip-10-0-0-124.eu-north-1.compute.internal ip-10-0-0-124 10.0.0.216 ip-10-0-0-216.eu-north-1.compute.internal ip-10-0-0-216 10.0.0.104 ip-10-0-0-104.eu-north-1.compute.internal ip-10-0-0-104 Even though the target machines are in the same subnet as our admin and they can access each other, we have to add them to the hosts file (/etc/hosts) for ceph-deploy to work properly. Ceph-deploy creates monitors by the provided hostname, so make sure it matches the actual hostname of the machines otherwise the monitors won’t be able to join the quorum and the deployment fails. Don’t forget to reboot the admin node for the changes to take effect. $ mkdir ceph-deploy $ cd ceph-deploy As a final step of the preparation, let’s create a dedicated folder as ceph-deploy will create multiple config and key files during the process. Deploy resources $ ceph-deploy new ip-10-0-0-124 ip-10-0-0-216 ip-10-0-0-104 The command ceph-deploy new creates the necessary files for the deployment. Pass it the hostnames of the monitor nodes, and it will create cepf.conf and ceph.mon.keyring along with a log file. The ceph-conf should look something like this [global] fsid = 0572e283-306a-49df-a134-4409ac3f11da mon_initial_members = ip-10-0-0-124, ip-10-0-0-216, ip-10-0-0-104 mon_host = 10.0.0.124,10.0.0.216,10.0.0.104 auth_cluster_required = cephx auth_service_required = cephx auth_client_required = cephx It has a unique ID called fsid, the monitor hostnames and addresses and the authentication modes. Ceph provides two authentication modes: none (anyone can access data without authentication) or cephx (key based authentication). The other file, the monitor keyring is another important piece of the puzzle, as all monitors must have identical keyrings in a cluster with multiple monitors. Luckily ceph-deploy takes care of the propagation of the key file during virtual deployments. $ ceph-deploy install --release nautilus ip-10-0-0-124 ip-10-0-0-216 ip-10-0-0-104 As you might have noticed so far, we haven’t installed ceph on the target nodes yet. We could do that one-by-one, but a more convenient way is to let ceph-deploy take care of the task. Don’t forget to specify the release of your choice, otherwise you might run into a mismatch between your admin and targets. $ ceph-deploy mon create-initial Finally, the first piece of the cluster is up and running! create-initial will deploy the monitors specified in ceph.conf we generated previously and also gather various key files. The command will only complete successfully if all the monitors are up and in the quorum. $ ceph-deploy admin ip-10-0-0-124 ip-10-0-0-216 ip-10-0-0-104 Executing ceph-deploy admin will push a Ceph configuration file and the ceph.client.admin.keyring to the /etc/ceph directory of the nodes, so we can use the ceph CLI without having to provide the ceph.client.admin.keyring each time to execute a command. At this point, we can take a peek at our cluster. Let’s SSH into a target machine (we can do it directly from the admin node thanks to agent forwarding) and run sudo ceph status. $ sudo ceph status cluster: id: 0572e283-306a-49df-a134-4409ac3f11da health: HEALTH_OK services: mon: 3 daemons, quorum ip-10-0-0-104,ip-10-0-0-124,ip-10-0-0-216 (age 110m) mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: Here we get a quick overview of what we have so far. Our cluster seems to be healthy and all three monitors are listed under services. Let’s go back to the admin and continue adding pieces. $ ceph-deploy mgr create ip-10-0-0-124 For luminous+ builds a manager daemon is required. It’s responsible for monitoring the state of the Cluster and also manages modules/plugins. Okay, now we have all the management in place, let’s add some storage to the cluster to make it actually useful, shall we? First, we have to find out (on each target machine) the label of the drive we want to use. To fetch the list of available disks on a specific node, run $ ceph-deploy disk list ip-10-0-0-104 Here’s a sample output: $ ceph-deploy osd create --data /dev/nvme1n1 ip-10-0-0-124 $ ceph-deploy osd create --data /dev/nvme1n1 ip-10-0-0-216 $ ceph-deploy osd create --data /dev/nvme1n1 ip-10-0-0-104 In my case the label was nvme1n1 on all 3 machines (courtesy of AWS), so to add OSDs to the cluster I just ran these 3 commands. At this point, our cluster is basically ready. We can run ceph status to see that our monitors, managers and OSDs are up and running. But nobody wants to SSH into a machine every time to check the status of the cluster. Luckily there’s a pretty neat dashboard that comes with Ceph, we just have to enable it. …Or at least that’s what I thought. The dashboard was introduced in luminous release and was further improved in mimic. However, currently we’re deploying nautilus, the latest version of Ceph. After trying the usual way of enabling the dashboard via a manager $ sudo ceph mgr module enable dashboard we get an error message saying Error ENOENT: all mgr daemons do not support module 'dashboard', pass --force to force enablement. Turns out, in nautilus the dashboard package is no longer installed by default. We can check the available modules by running $ sudo ceph mgr module ls and as expected, dashboard is not there, it comes in a form a separate package. So we have to install it first, luckily it’s pretty easy. $ sudo apt install -y ceph-mgr-dashboard Now we can enable it, right? Not so fast. There’s a dependency that has to be installed on all manager hosts, otherwise we get a slightly cryptic error message saying Error EIO: Module 'dashboard' has experienced an error and cannot handle commands: No module named routes. $ sudo apt install -y python-routes We’re all set to enable the dashboard module now. As it’s a public-facing page that requires login, we should set up a cert for SSL. For the sake of simplicity, I’ve just disabled the SSL feature. You should never do this in production, check out the official docs to see how to set up a cert properly. Also, we’ll need to create an admin user so we can log in to our dashboard. $ sudo ceph mgr module enable dashboard $ sudo ceph config set mgr mgr/dashboard/ssl false $ sudo ceph dashboard ac-user-create admin secret administrator By default, the dashboard is available on the host running the manager on port 8080. After logging in, we get an overview of the cluster status, and under the cluster menu, we get really detailed overviews of each running daemon. If we try to navigate to the Filesystems or Object Gateway tabs, we get a notification that we haven’t configured the required resources to access these features. Our cluster can only be used as a block storage right now. We have to deploy a couple of extra things to extend its usability. Quick detour: In case you’re looking for a company that can help you with Ceph, or DevOps in general, feel free to reach out to us at RisingStack! Using the Ceph filesystem Going back to our admin node, running $ ceph-deploy mds create ip-10-0-0-124 ip-10-0-0-216 ip-10-0-0-104 will create metadata servers, that will be inactive for now, as we haven’t enabled the feature yet. First, we need to create two RADOS pools, one for the actual data and one for the metadata. $ sudo ceph osd pool create cephfs_data 8 $ sudo ceph osd pool create cephfs_metadata 8 There are a couple of things to consider when creating pools that we won’t cover here. Please consult the documentation for further details. After creating the required pools, we’re ready to enable the filesystem feature $ sudo ceph fs new cephfs cephfs_metadata cephfs_data The MDS daemons will now be able to enter an active state, and we are ready to mount the filesystem. We have two options to do that, via the kernel driver or as FUSE with ceph-fuse. Before we continue with the mounting, let’s create a user keyring that we can use in both solutions for authorization and authentication as we have cephx enabled. There are multiple restrictions that can be set up when creating a new key specified in the docs. For example: $ sudo ceph auth get-or-create client.user mon 'allow r' mds 'allow r, allow rw path=/home/cephfs' osd 'allow rw pool=cephfs_data' -o /etc/ceph/ceph.client.user.keyring will create a new client key with the name user and output it into ceph.client.user.keyring. It will provide write access for the MDS only to the /home/cephfs directory, and the client will only have write access within the cephfs_data pool. Mounting with the kernel Now let’s create a dedicated directory and then use the key from the previously generated keyring to mount the filesystem with the kernel. $ sudo mkdir /mnt/mycephfs $ sudo mount -t ceph 13.53.114.94:6789:/ /mnt/mycephfs -o name=user,secret=AQBxnDFdS5atIxAAV0rL9klnSxwy6EFpR/EFbg== Attaching with FUSE Mounting the filesystem with FUSE is not much different either. It requires installing the ceph-fuse package. $ sudo apt install -y ceph-fuse Before we run the command we have to retrieve the ceph.conf and ceph.client.user.keyring files from the Ceph host and put the in /etc/ceph. The easiest solution is to use scp. $ sudo scp ubuntu@13.53.114.94:/etc/ceph/ceph.conf /etc/ceph/ceph.conf $ sudo scp ubuntu@13.53.114.94:/etc/ceph/ceph.client.user.keyring /etc/ceph/ceph.keyring Now we are ready to mount the filesystem. $ sudo mkdir cephfs $ sudo ceph-fuse -m 13.53.114.94:6789 cephfs Using the RADOS gateway To enable the S3 management feature of the cluster, we have to add one final piece, the rados gateway. $ ceph-deploy rgw create ip-10-0-0-124 For the dashboard, it’s required to create a radosgw-admin user with the system flag to enable the Object Storage management interface. We also have to provide the user’s access_key and secret_key to the dashboard before we can start using it. $ sudo radosgw-admin user create --uid=rg_wadmin --display-name=rgw_admin --system $ sudo ceph dashboard set-rgw-api-access-key <access_key> $ sudo ceph dashboard set-rgw-api-secret-key <secret_key> Using the Ceph Object Storage is really easy as RGW provides an interface identical to S3. You can use your existing S3 requests and code without any modifications, just have to change the connection string, access, and secret keys. Ceph Storage Monitoring The dashboard we’ve deployed shows a lot of useful information about our cluster, but monitoring is not its strongest suit. Luckily Ceph comes with a Prometheus module. After enabling it by running: $ sudo ceph mgr module enable prometheus A wide variety of metrics will be available on the given host on port 9283 by default. To make use of these exposed data, we’ll have to set up a prometheus instance. I strongly suggest running the following containers on a separate machine from your Ceph cluster. In case you are just experimenting (like me) and don’t want to use a lot of VMs, make sure you have enough memory and CPU left on your virtual machine before firing up docker, as it can lead to strange behaviour and crashes if it runs out of resources. There are multiple ways of firing up Prometheus, probably the most convenient is with docker. After installing docker on your machine, create a prometheus.yml file to provide the endpoint where it can access our Ceph metrics. # /etc/prometheus.yml scrape_configs: - job_name: 'ceph' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. static_configs: - targets: ['13.53.114.94:9283] Then launch the container itself by running: $ sudo docker run -p 9090:9090 -v /etc/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus Prometheus will start scraping our data, and it will show up on its dashboard. We can access it on port 9090 on its host machine. Prometheus dashboard is great but does not provide a very eye-pleasing dashboard. That’s the main reason why it’s usually used in pair with Graphana, which provides awesome visualizations for the data provided by Prometheus. It can be launched with docker as well. $ sudo docker run -d -p 3000:3000 grafana/grafana Grafana is fantastic when it comes to visualizations, but setting up dashboards can be a daunting task. To make our lives easier, we can load one of the pre-prepared dashboards, for example this one. Ceph Deployment: Lessons Learned & Next Up CEPH can be a great alternative to AWS S3 or other object storages when running in the public operating your service in the private cloud is simply not an option. The fact that it provides an S3 compatible interface makes it a lot easier to port other tools that were written with a “cloud first” mentality. It also plays nicely with Prometheus, thus you don’t need to worry about setting up proper monitoring for it, or you can swap it a more simple, more battle-hardened solution such as Nagios. In this article, we deployed CEPH to bare virtual machines, but you might need to integrate it into your KubernetesKubernetes (often abbreviated as K8s) offers a framework to run distributed systems efficiently. It's a platform that helps managing containerized workloads and services, and even takes care of scaling. Google open-sourced it in 2014. or Docker Swarm cluster. While it is perfectly fine to install it on VMs next to your container orchestration tool, you might want to leverage the services they provide when you deploy your CEPH cluster. If that is your use case, stay tuned for our next post covering CEPH where we’ll take a look at the black magic required to use CEPH on Docker Swarm and Kubernetes. In the next CEPH tutorial which we’ll release next week, we’re going to take a look at valid ceph storage alternatives with Docker or with Kubernetes. PS: Feel free to reach out to us at RisingStack in case you need help with Ceph or Ops in general! *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/ceph-storage-deployment-vm/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals ChatGPT use case examples for programming Last updated: March 3, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: Ferenc Hámori If you’re reading this post, you probably already know enough about large language models and other “AI” tools, so we can skip the intro. Despite the fact that the “AI is going to take our jobs” discourse proved to be an effective tool in the clickbait content creators toolbelt, I will not take this road. Instead of contributing to the moral panic about the supposedly inevitable replacement of white collar jobs, or pretending to be offended by a chatbot, I’ll help our readers to consider GPT-based products as tools that could be useful in a professional webdev setting. To do so, I asked some of my colleagues about their experiences of using GPT and various mutations of it – to help you get a more grounded understanding of their utility. In case you have an experience that you consider useful sharing with the RisingStack community, please share it though this form. I’ll drop the results / best ones in the article later on! Daniel’s ‘Code GPT’ vscode plugin review I’ve been pretty satisfied with GitHub Copilot. It does the job well, and it is priced reasonably. Still, after depleting the free tier, I decided to look for an open source alternative. TabNine is an honorable mention here, and a well established player, but based on my previous experience (about two years ago, mind you), it is clunky. Nowhere near the breeze of a dev experience you get from Copilot. But take heart, there is a staggering amount of plugins out there for VS Code, if you look for AI-based coding assistants. At the time of writing this, Code GPT is the winner by number of downloads, and number of (positive) votes, so I decided to give it a go. You can choose from a range of OpenAPI and Cohere models, with GPT-3 being the default. Features: 1, Code Generation from comment prompts The suggestions are relevant, and of quality. The plugin doesn’t offer code completion on the fly, unlike Copilot, but communicates with you in a new IDE pane it opens automatically instead. I like this feature, since I can pick the parts from the suggestion I liked, without bloating the code I’m working on, and having to delete the irrelevant lines. This behavior comes in handy with the other features as well. Let’s see those. 2, Unit Test generation While the results are often far from being complete, it saves me a lot of boilerplate code. It is also handy in reminding me of cases that I otherwise might have forgotten. For this feature to work well, adjust your max token length to a 1000 at least in the Settings, since a comprehensive test suite usually ends up quite verbose, and you’ll only get part of it with a tight quota. 3, Find Problems Your code review buddy. Once I feel I’m done with my work, a quick scan doesn’t take long before committing. While it often is straight out wrong about the ‘issues’ it points out, it doesn’t take long to scan through the suggestions, and catch mistakes before your real life reviewer does. 4, Refactor Save some time for your team lead for extra credits, and run Refactor against your code. Don’t expect miracles to happen, but often times it catches stuff that managed to sneak under your radar. Note: the default max token length won’t cut it here either. 5, Document and Explain Listed as two separate functionality in the documentation, it achieves essentially the same thing; provides a high level natural language description on what the highlighted peace of code does. I tend to use it less often, but it is a nice to have. 6, Ask CodeGPT I left it the last, but this is the most flexible feature of this plugin. It can achieve all previously mentioned functionalities with the right prompt, and more. Convert your .js to .ts, generate a README.md file from code, as suggested in the documentation, or just go ahead and ask for a recipe for a delicious apple pie, like you would from ChatGPT 🥧 My Conclusion: Code GPT offers many functionality that Copilot doesn’t, but lacks the thing Copilot is best at: inline code completion. So if you want to take the most out of AI, just use both, as these two tools complement each other really nice. Code GPT Might come handy if you’re just getting started with a new language or framework. The Explain feature helps double-check your gut feeling, or gives you the missing hint in the right direction. Bump up your max token length to at least a 1000, c’mon, it’s only ¢2 😉 An interesting alternative I might be trying in the future is ‘ChatGPT’ plugin (from either Tim Kmecl or Ali Gencay) that claims to be using the unofficial Chat GPT API, with all its superpowers. Further reading: – official site: https://www.codegpt.co/ – GitHub CoPilot vs ChatGPT: https://dev.to/ruppysuppy/battle-of-the-giants-github-copilot-vs-chatgpt-4oac – List of GitHub CoPilot alternatives: https://www.tabnine.com/blog/github-copilot-alternatives/ Olga on writing Mongo queries with ChatGPT I have used ChatGPT for more effective coding. It was really helpful for example with enhancing Mongo queries for more complex use cases as it suggested specific stages that worked for a use case, which would have definitely taken me more time to research and realize which stage and/or operator is ideal for this query. However all the answers it produces should be checked and not used blindly. I have not yet come across a case when the answer it provided didn’t need modification (though maybe it is due to the fact that I didn’t use it for easy things). I have also noticed that, if a question posted to ChatGPT includes many different parameters, in a lot of cases it will not take them all to consideration so one has to continue conversation and ensure all parameters are considered in the solution. Akos on using ChatGPT instead of StackOverflow I have been using ChatGPT since its inception and have found it to be a valuable tool in my daily work. With ChatGPT, I no longer have to spend hours searching and Googling for regex patterns on Stack Overflow. Instead, I simply write down what I want with the regex, and the tool returns the result, saving me a significant amount of time and effort. In addition to regex, I have also found ChatGPT to be a valuable tool when working on scrapers. Dealing with deeply nested selectors can be a challenge, and understanding how they work with scraping tools can take hours of research. But with ChatGPT, I can simply paste an example HTML and ask the tool to select what I want, saving me even more time and effort. However, it is important to use ChatGPT in moderation. Overusing the tool could lead to a decline in my problem-solving skills and make me too dependent on it. By setting limits, I can still benefit from ChatGPT’s time-saving features while maintaining my critical thinking skills. In conclusion, ChatGPT has been a powerful tool in my work, saving me time and effort in regex and scraping tasks. However, it is important to use it in moderation and not let it compromise my problem-solving skills. Me, Navigating the hallucinations of chatGPT I work with a lot of text: business documents, project proposals, contracts, articles, marketing materials, you name it. I’ve been fascinated by the emergence of “AI” tools in 2022 and both the entertainment factor and the usefulness they can provide. All big areas – text, voice, image – had huge releases that made the internet a very interesting place again after the boring years of the web3 / crypto / nft / metaverse nonsense. Suddenly I felt the rush of discovering the internet in the early 2000’s. “Wow, I can do that?!” was a sentence that got repeated a lot this year. The Dall-E2 / Stable Diffusion / Whisper releases were more than enough to shake up the web, but ChatGPT was the one that really disrupted the web. The first good conversational “AI” that everyone could use, and for free (for now). At times it can be so convincing that people forget that it is just a very good bullshitter. For example when I ask it about RisingStack, I get the following answer: “RisingStack is a software development and consulting company that specializes in building scalable, high-performance systems using Node.js, Kubernetes, and other modern web technologies. The company was founded in 2013 and is headquartered in Budapest, Hungary, with additional offices in London, United Kingdom, and New York, United States. RisingStack offers a range of services, including software development, architecture design, cloud infrastructure management, performance optimization, and training. They also develop open-source tools such as Trace, a Node.js performance monitoring and debugging tool, and Pino, a Node.js logging library. RisingStack has worked with a variety of clients, ranging from small startups to large enterprises, across a range of industries, including fintech, healthcare, e-commerce, and media. They are known for their expertise in Node.js and their commitment to delivering high-quality software solutions.” Most of this text is usable, but it contains a lot of outright lies. It is important to keep in mind that GPT hallucinates. It has no idea about what is true and what is not. It pretends to know things, but it’s just making things up word by word. In this case: RisingStack was founded in 2014, and we never had a London office. Trace was sunset like 6 years ago, and Pino has nothing to do with us. Anyways, I find it really useful when I need to generate / rephrase / improve text. It is only valid as a Google replacement if you can confidently tell if it’s right or wrong, so “geepeetee-ing” something is not really that helpful right now. I already used it to write contracts, blog posts (not this one though), business proposals. It also brought in new clients, as just in the past couple of weeks we used it to.. Automatically generate product names and descriptions for webshops Create easy-read text for children with disabilities Perform sentiment analysis and write answers automatically to customer reviews Currently chatGPT has a lame writing style by default. It’s very formulaic. I’ve seen so much of it that I believe I can spot it 8 out of 10 times right away. It lies a lot, and I wasn’t able to get anything guitar-related useful out of it, despite the fact that the training material probably has a couple million tabs in it. Anyways, here are my not-so-hot takes to about it: You really need to carefully double check everything you generate. On the surface most of it might look good enough, but that’s just making it easier for everyone to get lazy with it. “AI” won’t replace jobs, instead, it will just improve productivity. As Photoshop is a better brush, GPT should be thought of as a better text/code editor. Most of the office jobs are about collaboration anyways, not typing on a keyboard. Artists won’t get replaced en masse. You won’t be able to prompt an engine to generate artwork in de Goya’s style, if cave paintings are the apex of your visual art knowledge. Taste will be very important to stand out when the web gets flooded with endless mediocre “art”. Also.. It will be interesting to see how the “poisoning the well” problem will affect these models. The continuous retraining of the “AI” on already “AI generated” content will cause a big decline in the quality of these services, in case they won’t be able to filter them out… While they are working on making the generated content so good that it gets mistaken for genuine human creation. It’s a bit scary to think about how Microsoft will dominate this space through its OpenAI investment. Despite the genius branding, it is not open at all, and will cost a lot of money without serious competitors or general access to free-to-use alternatives (like Stable Diffusion for images). Most of the coverage GPT gets nowadays is about people gaming the engine to finally say something “bad”, then pretending to be offended, even more so, scared of it! This kind of AI ethics/alignment discourse is incredibly dull and boring, imho.. Although the adversarial aspect is very interesting. Poisoning generally available chatbots training data will be a prime trolling activity, while convincing chatbots to spill their carefully crafted secret sauce prompts is something that needs to be continuously prevented. I was first skeptical about prompt engineering as an emerging “profession”, but seeing how building products on top of GPT3 requires proper prompting and safeguards to make the end result consistently useful for end users, I can see it happening. Also, when you build something LLM driven, you need to be aware that hostile users, trolls, competitors, etc.. will try to game your product to ramp up your cloud costs or cause reputational harm. This tweet really gets it. This is already happening. Most of the “AI-driven” products are just purpose-repacked custom prompters calling GPT3 through an API, with a fancy UI. Anyways, I’m looking forward to seeing what kind of GPT driven products we’ll make for our clients, and how the internet will change in general. I’m curious about your experience with chatGPT, so please share it with me through this short form! Cheers, Ferenc *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/chatgpt-use-case-examples-for-programming/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals Distributed Load Testing with Jmeter Last updated: June 26, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: Janos Kubisch Many of you have probably used apache Jmeter for load testing before. Still, it is easy to run into the limits imposed by running it on just one machine when trying to make sure that our API will be able to serve hundreds of thousands or even millions of users. We can get around this issue by deploying and running our tests to multiple machines in the cloud. In this article, we will take a look at one way to distribute and run Jmeter tests along multiple droplets on DigitalOcean using Terraform, AnsibleAnsible is an open-source software provisioning, configuration management, and application-deployment tool. It enables Infrastructure-as-Code (IaC), meaning that it can handle the state of infrastructure through idempotent changes, defined with an easily readable, domain-specific language instead of relying on Bash scripts., and a little bit of bash scripting to automate the process as much as possible. Background: During the COVID19 outbreak induced lockdowns, we’ve been tasked by a company (who builds an e-learning platform primarily for schools) to build out an infrastructure that is: geo redundant, supports both single and multi tenant deployments , can be easily scaled to serve at least 1.5 million users in huge bursts, and runs on-premises. To make sure the application is able to handle these requirements, we needed to set up the infrastructure, and model a reasonably high burst in requests to get an idea about the load the application and its underlying infrastructure is able to serve. In this article, we’ll share practical advice and some of the scripts we used to automate the load-testing process using Jmeter, Terraform and Ansible. Let’s Start! Have these tools installed before you begin! brew install ansible brew install terraform brew install jmeter You can just run them from your own machine. The full codebase is available on Github at RisingStack/distributed-loadtests-jmeter for your convenience. Why do we use Jmeter for distributed load testing? Jmeter is not my favorite tool for load testing owing mostly to the fact that scripting it is just awkward. But looking at the other tools that support distribution, it seems to be the best free one for now. K6 looks good, but right now it does not support distribution outside the paid, hosted version. Locust is another interesting one, but it’s focusing too much on random test picking, and if that’s not what I’m looking for, it is quite awkward to use as well – just not flexible enough right now. So, back to Jmeter! Terraform is infrastructure as code, which allows us to describe the resources we want to use in our deployment and configure the droplets so we have them ready for running some tests. This will, in turn, be deployed by Ansible to our cloud service provider of choice, DigitalOcean – though with some changes, you can make this work with any other provider, as well as your on-premise machines if you wish so. Deploying the infrastructure There will be two kinds of instances we’ll use: primary, of which we’ll have one coordinating the testing, and runners, that we can have any number of. In the example, we’re going to go with two, but we’ll see that it is easy to change this when needed. You can check the variables.tf file to see what we’ll use. You can use these to customise most aspects of the deployment to fit your needs. This file holds the vars that will be plugged into the other template files – main.tf and provider.tf. The one variable you’ll need to provide to Terraform for the example setup to work is your DigitalOcean api token, that you can export like this from the terminal: export TF_VAR_do_token=DO_TOKEN Should you wish to change the number of test runner instances, you can do so by exporting this other environment variable: export TF_VAR_instance_count=2 You will need to generate two ssh key pairs, one for the root user, and one for a non-privileged user. These will be used by Ansible, which uses ssh to deploy the testing infrastructure as it is agent-less. We will also use the non-privileged user when starting the tests for copying over files and executing commands on the primary node. The keys should be set up with correct permissions, otherwise, you’ll just get an error. Set the permissions to 600 or 700 like this: chmod 600 /path/to/folder/with/keys/* To begin, we should open a terminal in the terraform folder, and call terraform init which will prepare the working directory. Thisl needs to be called again if the configuration changes. You can use terraform plan that will output a summary of what the current changes will look like to the console to double-check if everything is right. At the first run, it will be what the deployment will look like. Next, we call terraform apply which will actually apply the changes according to our configuration, meaning we’ll have our deployment ready when it finishes! It also generates a .tfstate file with all the information about said deployment. If you wish to dismantle the deployment after the tests are done, you can use terraform destroy. You’ll need the .tfstate file for this to work though! Without the state file, you need to delete the created droplets by hand, and also remove the ssh key that has been added to DigitalOcean. Running the Jmeter tests The shell script we are going to use for running the tests is for convenience – it consists of copying the test file to our primary node, cleaning up files from previous runs, running the tests, and then fetching the results. #!/bin/bash set -e # Argument parsing, with options for long and short names for i in \"$@\" do case $i in -o=*|--out-file=*) # i#*= This removes the shortest substring ending with # '=' from the value of variable i - leaving us with just the # value of the argument (i is argument=value) OUTDIR=\"${i#*=}\" shift ;; -f=*|--test-file=*) TESTFILE=\"${i#*=}\" shift ;; -i=*|--identity-file=*) IDENTITYFILE=\"${i#*=}\" shift ;; -p=*|--primary-ip=*) PRIMARY=\"${i#*=}\" shift ;; esac done # Check if we got all the arguments we'll need if [ -z \"$TESTFILE\" ] || [ ! -f \"$TESTFILE\" ]; then echo \"Please provide a test file\" exit 1 fi if [ -z \"$OUTDIR\" ]; then echo \"Please provide a result destination directory\" exit 1 fi if [ -z \"$IDENTITYFILE\" ]; then echo \"Please provide an identity file for ssh access\" exit 1 fi if [ -z \"$PRIMARY\" ]; then PRIMARY=$(terraform output primary_address) fi # Copy the test file to the primary node scp -i \"$IDENTITYFILE\" -o IdentitiesOnly=yes -oStrictHostKeyChecking=no \"$TESTFILE\" \"runner@$PRIMARY:/home/runner/jmeter/test.jmx\" # Remove files from previous runs if any, then run the current test ssh -i \"$IDENTITYFILE\" -o IdentitiesOnly=yes -oStrictHostKeyChecking=no \"runner@$PRIMARY\" << \"EOF\" rm -rf /home/runner/jmeter/result rm -f /home/runner/jmeter/result.log cd jmeter/bin ; ./jmeter -n -r -t ../test.jmx -l ../result.log -e -o ../result -Djava.rmi.server.hostname=$(hostname -I | awk ' {print $1}') EOF # Get the results scp -r -i \"$IDENTITYFILE\" -o IdentitiesOnly=yes -oStrictHostKeyChecking=no \"runner@$PRIMARY\":/home/runner/jmeter/result \"$OUTDIR\" Running the script will require the path to the non-root ssh key. The call will look something like this: bash run.sh -i=/path/to/non-root/ssh/key -f=/path/to/test/file -o=/path/to/results/dir You can also supply the IP of the primary node using -p= or --primary-ip= in case you don’t have access to the .tfstate file. Otherwise, the script will ask terraform for the IP. Jmeter will then take care of distributing the tests across the runner nodes, and it will aggregate the data when they finish. The only thing we need to keep in mind is that the number of users we set for our test to use will not be split but will be multiplied. As an example, if you set the user count to 100, each runner node will then run the tests with 100 users. And that’s how you can use Terraform and Ansible to run your distributed Jmeter tests on DigitalOcean! Check this page for more on string manipulation in bash. Looking for DevOps & Infra Experts? In case you’re looking for expertise in infrastructure related matters, I’d recommend to read our articles and ebooks on the topic, and to check out our various service pages: * Services Infrastructure Assessment and Code Review Services Kubernetes Consulting & Training Services An early draft of this article was written by Mate Boer, and then subsequently rewritten by Janos Kubisch – both engineers at RisingStack. *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/distributed-load-testing-with-jmeter/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals History of JavaScript on a Timeline Last updated: May 8, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: RisingStack Engineering In the early 1990s, Brendan Eich was working on a project at Netscape Communications Corporation. He needed a scripting language for web pages that would be easy to use, so he created one himself. He called it JavaScript. And the rest, as they say, is history. In this blog post, we’ll take a look at the history of JavaScript on a timeline. We’ll see how it has evolved over the years and what new features have been added along the way. So sit back and enjoy learning about one of the most popular programming languages in the world! 1994-1998: The Netscape era On December 15, 1994, Netscape Communications Corporation released the Netscape Navigator 1.0 web browser. Brendan Eich created the very first version of JavaScript, codenamed “Mocha”, then later (still internally) renamed to LiveScript “Netscape and Sun announce JavaScript, the open, cross-platform object scripting language for enterprise networks and the internet” Microsoft introduced JScript in Internet Explorer to compete with Netscape. Netscape 2 was released with JavaScript 1.0 Netscape submitted JavaScript to Ecma International, as the starting point for a standard specification. Official release of the first ECMAScript language specification. 1999-2007: The showdown of Internet Explorer VS Mozilla Firefox Microsoft releases Internet Explorer 5, that uses even more proprietary technology than before. ECMAScript 2: Editorial changes to align ECMA-262 with the standard ISO/IEC 16262 ECMAScript 3: do-while, regular expressions, new string methods (concat, match, replace, slice, split with a regular expression, etc.), exception handling, and more Firefox is released to compete with Internet Explorer. Jesse James Garrett released a white paper in which he coined the term Ajax. 2008-2012: Netscape died, and Google Chrome was created Netscape Navigator: end of life ECMAScript 4 is officially abandoned. Google releases the Chrome browser, the fastest web browser at the time. Node.js was created by Ryan Dahl ECMAScript 5 (formerly ECMAScript 3.1), that adds a strict mode, getters and setters, new array methods, support for JSON, and more. TypeScript: a language for application-scale JavaScript development 2013-2014: from ASM.js to WebAssembly ASM.js has been released React, a JavaScript library for building user interfaces “Disable Javascript” option removed in Firefox 23 Facebook Launches Flow, Static Type Checker for JavaScript 2015-2020: the rise of Node.js Introduction of the Node.js Foundation ECMAScript 6 (ES2015) is released. WebAssembly Object.observe withdrawn from TC39 Microsoft Edge’s JavaScript engine to go open-source ECMAScript 2016 Language Specification ECMAScript 2017 Language Specification ECMA TC39: “SmooshGate” was officially resolved by renaming flatten to flat ECMAScript 2018 Language Specification JavaScript is now required to sign in to Google ECMAScript modules in Node.js ECMAScript 2019 Language Specification QuickJS JavaScript Engine 2020-2022: Deno is created and Internet Explorer is officially retired Deno: initial release ECMAScript 2020 Language Specification ECMAScript 2021 Language Specification Deno joins TC39 Internet Explorer 11 has retired and is officially out of support *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/history-of-javascript-on-a-timeline/"
  },
  {
    "text": "* & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * ** & Consulting De*nsul*ent & Code Reviews Trainings & Education Trainings Why learn from us? * *ngular Node.js Fundamentals Sometimes you do need Kubernetes! But how should you decide? Last updated: September 8, 2023 *e.js Consulting De*ting 24.7 Node.js Support Infrastructu*i*es Learn more at risingstack.com * In this article: Tamas Kadlecsik At RisingStack, we help companies to adopt cloud-native technologies, or if they have already done so, to get the most mileage out of them. Recently, I’ve been invited to Google DevFest to deliver a presentation on our experiences working with KubernetesKubernetes (often abbreviated as K8s) offers a framework to run distributed systems efficiently. It's a platform that helps managing containerized workloads and services, and even takes care of scaling. Google open-sourced it in 2014.. Below I talk about an online learning and streaming platform where the decision to use Kubernetes has been contested both internally and externally since the beginning of its development. The application and its underlying infrastructure were designed to meet the needs of the regulations of several countries: The app should be able to run on-premises, so students’ data could never leave a given country. Also, the app had to be available as a SaaS product as well. It can be deployed as a single-tenant system where a business customer only hosts one instance serving a handful of users, but some schools could have hundreds of users. Or it can be deployed as a multi-tenant system where the client is e.g. a government and needs to serve thousands of schools and millions of users. Learn When to Use Kubernetes: Get Our Case Study! Enter your email to subscribe to our newsletter and we’ll send you a link to download the case study. DOWNLOAD NOW The application itself was developed by multiple, geographically scattered teams, thus a MicroservicesMicroservices are not a tool, rather a way of thinking when building software applications. Let's begin the explanation with the opposite: if you develop a single, self-contained application and keep improving it as a whole, it's usually called a monolith. Over time, it's more and more difficult to maintain and update it without breaking anything, so the development cycle may... architecture was justified, but both the distributed system and the underlying infrastructure seemed to be an overkill when we considered the fact that during the product’s initial entry, most of its customers needed small instances. Was Kubernetes suited for the job, or was it an overkill? Did our client really need Kubernetes? Let’s figure it out. (Feel free to check out the video presentation, or the extended article version below!) Let’s talk a bit about Kubernetes itself! Kubernetes is an open-source container orchestration engine that has a vast ecosystem. If you run into any kind of problem, there’s probably a library somewhere on the internet that already solves it. But Kubernetes also has a daunting learning curve, and initially, it’s pretty complex to manage. Cloud ops / infrastructure engineering is a complex and big topic in and of itself. Kubernetes does not really mask away the complexity from you, but plunges you into deep water as it merely gives you a unified control plane to handle all those moving parts that you need to care about in the cloud. So, if you’re just starting out right now, then it’s better to start with small things and not with the whole package straight away! First, deploy a VM in the cloud. Use some PaaS or FaaS solutions to play around with one of your apps. It will help you gradually build up the knowledge you need on the journey. So you want to decide if Kubernetes is for you. First and foremost, Kubernetes is for you if you work with containers! (It kinda speaks for itself for a container orchestration system). But you should also have more than one service or instance. Kubernetes makes sense when you have a huge microservice architecture, or you have dedicated instances per tenant having a lot of tenants as well. Also, your services should be stateless, and your state should be stored in databases outside of the cluster. Another selling point of Kubernetes is the fine gradient control over the network. And, maybe the most common argument for using Kubernetes is that it provides easy scalability. Okay, and now let’s take a look at the flip side of it. Kubernetes is not for you if you don’t need scalability! If your services rely heavily on disks, then you should think twice if you want to move to Kubernetes or not. Basically, one disk can only be attached to a single node, so all the services need to reside on that one node. Therefore you lose node auto-scaling, which is one of the biggest selling points of Kubernetes. For similar reasons, you probably shouldn’t use k8s if you don’t host your infrastructure in the public cloud. When you run your app on-premises, you need to buy the hardware beforehand and you cannot just conjure machines out of thin air. So basically, you also lose node auto-scaling, unless you’re willing to go hybrid cloud and bleed over some of your excess load by spinning up some machines in the public cloud. If you have a monolithic application that serves all your customers and you need some scaling here and there, then cloud service providers can handle it for you with autoscaling groups. There is really no need to bring in Kubernetes for that. Let’s see our Kubernetes case-study! Maybe it’s a little bit more tangible if we talk about an actual use case, where we had to go through the decision making process. Online Learning Platform is an application that you could imagine as if you took your classroom and moved it to the internet. You can have conference calls. You can share files as handouts, you can have a whiteboard, and you can track the progress of your students. This project started during the first wave of the lockdowns around March, so one thing that we needed to keep in mind is that time to market was essential. In other words: we had to do everything very, very quickly! This product targets mostly schools around Europe, but it is now used by corporations as well. So, we’re talking about millions of users from the point we go to the market. The product needed to run on-premise, because one of the main targets were governments. Initially, we were provided with a proposed infrastructure where each school would have its own VM, and all the services and all the databases would reside in those VMs. Handling that many virtual machines, properly handling rollouts to those, and monitoring all of them sounded like a nightmare to begin with. Especially if we consider the fact that we only had a couple of weeks to go live. After studying the requirements and the proposal, it was time to call the client to.. Discuss the proposed infrastructure. So the conversation was something like this: “Hi guys, we would prefer to go with Kubernetes because to handle stuff at that scale, we would need a unified control plane that Kubernetes gives us.” \"Yeah, sure, go for it.\" And we were happy, but we still had a couple of questions: “Could we, by any chance, host it on the public cloud?” \"Well, no, unfortunately. We are negotiating with European local governments and they tend to be squeamish about sending their data to the US. \" Okay, anyways, we can figure something out… “But do the services need filesystem access?” \"Yes, they do.\" Okay, crap! But we still needed to talk to the developers so all was not lost. Let’s call the developers! It turned out that what we were dealing with was an usual microservice-based architecture, which consisted of a lot of services talking over HTTP and messaging queues. Each service had its own database, and most of them stored some files in Minio. In case you don’t know it, Minio is an object storage system that implements the S3 API. Now that we knew the fine-grained architectural layout, we gathered a few more questions: “Okay guys, can we move all the files to Minio?” \"Yeah, sure, easy peasy.\" So, we were happy again, but there was still another problem, so we had to call the hosting providers: “Hi guys, do you provide hosted Kubernetes?” \"Oh well, at this scale, we can manage to do that!\" So, we were happy again, but.. Just to make sure, we wanted to run the numbers! Our target was to be able to run 60 000 schools on the platform in the beginning, so we had to see if our plans lined up with our limitations! We shouldn’t have more than 150 000 total pods! 10 (pod/tenant) times 6000 tenants is 60 000 Pods. We’re good! We shouldn’t have more than 300 000 total containers! It’s one container per pod, so we’re still good. We shouldn’t have more than 100 pods per node and no more than 5 000 nodes. Well, what we have is 60 000 pods over 100 pod per node. That’s already 6 000 nodes, and that’s just the initial rollout, so we’re already over our 5 000 nodes limit. Okay, well… Crap! But, is there a solution to this? Sure, it’s federation! We could federate our Kubernetes clusters.. ..and overcome these limitations. We have worked with federated systems before, so Kubernetes surely provides something for that, riiight? Well yeah, it does… kind of. It’s the stable Federation v1 API, which is sadly deprecated. Then we saw that Kubernetes Federation v2 is on the way! It was still in alpha at the time when we were dealing with this issue, but the GitHub page said it was rapidly moving towards beta release. By taking a look at the releases page we realized that it had been overdue by half a year by then. Since we only had a short period of time to pull this off, we really didn’t want to live that much on the edge. So what could we do? We could federate by hand! But what does that mean? In other words: what could have been gained by using KubeFed? Having a lot of services would have meant that we needed a federated Prometheus and Logging (be it Graylog or ELK) anyway. So the two remaining aspects of the system were rollout / tenant generation, and manual intervention. Manual intervention is tricky. To make it easy, you need a unified control plane where you can eyeball and modify anything. We could have built a custom one that gathers all information from the clusters and proxies all requests to each of them. However, that would have meant a lot of work, which we just did not have the time for. And even if we had the time to do it, we would have needed to conduct a cost/benefit analysis on it. The main factor in the decision if you need a unified control plane for everything is scale, or in other words, the number of different control planes to handle. The original approach would have meant 6000 different planes. That’s just way too much to handle for a small team. But if we could bring it down to 20 or so, that could be bearable. In that case, all we need is an easy mind map that leads from services to their underlying clusters. The actual route would be something like: Service -> Tenant (K8s Namespace) -> Cluster. The Service -> Namespace mapping is provided by Kubernetes, so we needed to figure out the Namespace -> Cluster mapping. This mapping is also necessary to reduce the cognitive overhead and time of digging around when an outage may happen, so it needs to be easy to remember, while having to provide a more or less uniform distribution of tenants across Clusters. The most straightforward way seemed to be to base it on Geography. I’m the most familiar with Poland’s and Hungary’s Geography, so let’s take them as an example. Poland comprises 16 voivodeships, while Hungary comprises 19 counties as main administrative divisions. Each country’s capital stands out in population, so they have enough schools to get a cluster on their own. Thus it only makes sense to create clusters for each division plus the capital. That gives us 17 or 20 clusters. So if we get back to our original 60 000 pods, and 100 pod / tenant limitation, we can see that 2 clusters are enough to host them all, but that leaves us no room for either scaling or later expansions. If we spread them across 17 clusters – in the case of Poland for example – that means we have around 3.500 pods / cluster and 350 nodes, which is still manageable. This could be done in a similar fashion for any European country, but still needs some architecting when setting up the actual infrastructure. And when KubeFed becomes available (and somewhat battle tested) we can easily join these clusters into one single federated cluster. Great, we have solved the problem of control planes for manual intervention. The only thing left was handling rollouts.. As I mentioned before, several developer teams had been working on the services themselves, and each of them already had their own Gitlab repos and CIs. They already built their own Docker images, so we simply needed a place to gather them all, and roll them out to Kubernetes. So we created a GitOps repo where we stored the helm charts and set up a GitLab CI to build the actual releases, then deploy them. From here on, it takes a simple loop over the clusters to update the services when necessary. The other thing we needed to solve was tenant generation. It was easy as well, because we just needed to create a CLI tool which could be set up by providing the school’s name, and its county or state. That’s going to designate its target cluster, and then push it to our Gitops repo, and that basically triggers the same rollout as new versions. We were almost good to go, but there was still one problem: on-premises. Although our hosting providers turned into some kind of public cloud (or something we can think of as public clouds), we were also targeting companies who want to educate their employees. Huge corporations – like a Bank – are just as squeamish about sending their data out to the public internet as governments, if not more.. So we needed to figure out a way to host this on servers within vaults completely separated from the public internet. In this case, we had two main modes of operation. One is when a company just wanted a boxed product and they didn’t really care about scaling it. And the other one was where they expected it to be scaled, but they were prepared to handle this. In the second case, it was kind of a bring your own database scenario, so you could set up the system in a way that we were going to connect to your database. And in the other case, what we could do is to package everything — including databases — in one VM, in one Kubernetes cluster. But! I just wrote above that you probably shouldn’t use disks and shouldn’t have databases within your cluster, right? However, in that case, we already had a working infrastructure. Kubernetes provided us with infrastructure as code already, so it only made sense to use that as a packaging tool as well, and use Kubespray to just spray it to our target servers. It wasn’t a problem to have disks and DBs within our cluster because the target were companies that didn’t want to scale it anyway. So it’s not about scaling. It is mostly about packaging! Previously I told you, that you probably don’t want to do this on-premises, and this is still right! If that’s your main target, then you probably shouldn’t go with Kubernetes. However, as our main target was somewhat of a public cloud, it wouldn’t have made sense to just recreate the whole thing – basically create a new product in a sense – for these kinds of servers. So as it is kind of a spin-off, it made sense here as well as a packaging solution. Basically, I’ve just given you a bullet point list to help you determine whether Kubernetes is for you or not, and then I just tore it apart and threw it into a basket. And the reason for this is – as I also mentioned: Cloud ops is difficult! There aren’t really one-size-fits-all solutions, so basing your decision on checklists you see on the internet is definitely not a good idea. We’ve seen that a lot of times where companies adopt Kubernetes because it seems to fit, but when they actually start working with it, it turns out to be an overkill. If you want to save yourself about a year or two of headache, it’s a lot better to first ask an expert, and just spend a couple of hours or days going through your use cases, discussing those and save yourself that year of headache. In case you’re thinking about adopting Kubernetes, or getting the most out of it, don’t hesitate to reach out to us at info@risingstack.com, or by using the contact form below! *i*es L*i*es *e.js Consulting DevOps, SRE & Cloud Consul*ent & Code Re* for Software Developers * Building Complex Apps with A*.",
    "url": "https://blog.risingstack.com/kubernetes-case-study-learning-platform/"
  }
]
